{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GDELT_Download.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"I2M6jRpYUmvq","colab_type":"text"},"source":["# GDELT Python Script\n","\n","Run each cell from top to bottom to execeute the code. You might need to copy this file to your private Google Drive as I am not sure if we can run the same file in parallel. You might also first need to link the AI Capstone folder to your private Drive to access it via the code. To do this right click on the Capstone folder and create a short cut in \"My Drive\". The runtime type can be changed by clicking *Runtime -> Change runtime* type in the top menu and selecting the desired hardware acceleration. I think setting the accelerator to TPU leads to the fastest performance in our case.\n","When you are done you can close the session to unmount your drive by clicking on *Runtime -> Manage sessions* and terminating the session.\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CieObXA5UCib","colab_type":"text"},"source":["Mount Google drive folder by running this cell. Click on the link and add your google account to get the access token. Enter the token into the prompt in the code cell."]},{"cell_type":"code","metadata":{"id":"b8rHAH6Li8A6","colab_type":"code","colab":{}},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-M7bSu2U2pK","colab_type":"text"},"source":["Install GDELT to the runtime. This step has to be done every time you open this script as the runtime gets set up from scratch each time."]},{"cell_type":"code","metadata":{"id":"N6bHfDnhjDXq","colab_type":"code","colab":{}},"source":["!pip install gdelt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NoDPAN62VPZi","colab_type":"text"},"source":["Import the python librariers necessary for the next steps"]},{"cell_type":"code","metadata":{"id":"6euROfrPjNzZ","colab_type":"code","colab":{}},"source":["import gdelt\n","import pandas as pd\n","import time\n","from pathlib import Path\n","from dateutil import relativedelta\n","from datetime import timedelta, date, datetime\n","from concurrent.futures import ProcessPoolExecutor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rTyjhwj9VlJK","colab_type":"text"},"source":["This function gets called by `parallel_gdelt` and loaads data for one single day from the global knowledge graph (GKG) of GDELT. Currently the filters are set so the location is Germany AND article urls end in .de, .net or .com AND either the url contains \"brauer\" OR the field AllNames contains \"brewery\" or \"breweries\". Information about the datafields/column names can be found in the GKG documentation. The function returns a pandas dataframe containing the filtered GDELT data. We are using version 2 of the GKG containing data from february 2015 onwards as this version provides articles in other languages (German in our case), which is why `translation` is set to True in the search statement. \n","\n","> [Link to GKG documentation](http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf)"]},{"cell_type":"code","metadata":{"id":"o_ZdKPK59q95","colab_type":"code","colab":{}},"source":["def load_single_date(single_date):\n","  gd = gdelt.gdelt()\n","  try:\n","    gkg = gd.Search(single_date.strftime(\"%Y-%m-%d\"), table='gkg', output='df', coverage=True, translation=True)\n","    result = gkg[gkg['Locations'].str.contains(\"Germany\", na=False, case=False)\n","              & (gkg['SourceCommonName'].str.contains(\".de\", na=False, case=False)\n","              | gkg['SourceCommonName'].str.contains(\".net\", na=False, case=False)\n","              | gkg['SourceCommonName'].str.contains(\".com\", na=False, case=False))\n","              & (gkg['DocumentIdentifier'].str.contains(\"brauer\", na=False, case=False)   \n","              | gkg['AllNames'].str.contains(\"brewery\", na=False, case=False)\n","              | gkg['AllNames'].str.contains(\"breweries\", na=False, case=False))\n","              & (gkg['V2Persons'].str.contains(\"brauer\",na=False, case=False)== False)\n","              ]\n","    return result\n","  except:\n","      pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Maqa4TDrYMsK","colab_type":"text"},"source":["This function runs `load_single_date` for a specified date range (in our case one month). It is using multiprocessing to run multiple days at once. This is not super effective as we can only run two days at a time over Google colab but at least it provides some kind of parallelization. The GDELT data for the month is stored in a dataframe and saved to Google Drive as a csv file which name contains the year and the month of the downloaded data."]},{"cell_type":"code","metadata":{"id":"7iWhz596qfDz","colab_type":"code","colab":{}},"source":["def parallel_gdelt(start_date):\n","  end_date = start_date + relativedelta.relativedelta(months=1, day=1)\n","  if end_date > datetime.now().date():\n","    end_date = datetime.now().date()\n","  output_file = Path(f'/content/drive/My Drive/AI Capstone/GDELT/Data/brauerei_{start_date.year}_{start_date.month}.csv')\n","  e = ProcessPoolExecutor()\n","\n","  df_list = [fr for i, fr in zip(pd.date_range(start_date, end_date), e.map(load_single_date, pd.date_range(start_date, end_date)[:-1]))]\n","  df = pd.concat(df_list, ignore_index=True)\n","  df.to_csv(output_file)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Exvy5GYUmY6u","colab_type":"code","colab":{}},"source":["def load_gdelt_csv_append(start_date):\n","    gd = gdelt.gdelt()\n","    end_date = start_date + relativedelta.relativedelta(months=1, day=1)\n","    if end_date > datetime.now().date():\n","      end_date = datetime.now().date()\n","    output_file = Path(f'/content/drive/My Drive/AI Capstone/GDELT/Data/brauerei_{start_date.year}_{start_date.month}.csv')\n","    for i in pd.date_range(start_date, end_date)[:-1]:\n","      result = load_single_date(i)\n","      if output_file.is_file():\n","        result.to_csv(output_file, mode='a', header=False)\n","      else:\n","        result.to_csv(output_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2Y-dK0OaCcx","colab_type":"text"},"source":["Run either of the next two code blocks. The first one loads the data for just one month and the second one iterates over one year. When changing the date only change the year and the month and leave the day as the first of the month. The warnings about GDELT not returning data for a specific time can be ignored as GDELT tries to access data in 15 minute intervals from each day and not every interval returns new data."]},{"cell_type":"code","metadata":{"id":"KsAyYFGettWC","colab_type":"code","colab":{}},"source":["# single month\n","start_time = time.time()\n","start_date = date(2015, 1, 1)\n","parallel_gdelt(start_date)\n","end_time = time.time()\n","print(end_time - start_time)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xgLa76j5Rd3c","colab_type":"code","colab":{}},"source":["# whole year\n","for month in range (3, 13):\n","  start_date = date(2015, month, 1)\n","  parallel_gdelt(start_date)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PyPgHd1lmeYo","colab_type":"code","colab":{}},"source":["# hopefully fixes the RAM issue, at least as a temporary solution\n","start_time = time.time()\n","start_date = date(2018, 3, 1)\n","load_gdelt_csv_append(start_date)\n","end_time = time.time()\n","print(end_time - start_time)"],"execution_count":null,"outputs":[]}]}